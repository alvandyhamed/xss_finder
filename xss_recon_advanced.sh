#!/bin/bash

RED="\033[0;31m"
GREEN="\033[0;32m"
BLUE="\033[0;34m"
YELLOW="\033[1;33m"
NC="\033[0m"

REQUIRED_TOOLS=("assetfinder" "httpx" "gau" "waybackurls" "paramspider")

for tool in "${REQUIRED_TOOLS[@]}"; do
    if ! command -v $tool &> /dev/null; then
        echo -e "${RED}âŒ Ø§Ø¨Ø²Ø§Ø± $tool Ù†ØµØ¨ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ù†ØµØ¨Ø´ Ú©Ù† Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù†.${NC}"
        exit 1
    fi
done

read -p "ğŸ” Ù„Ø·ÙØ§Ù‹ Ø¯Ø§Ù…Ù†Ù‡ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù† (Ù…Ø«Ø§Ù„: example.com): " domain

start_time=$(date +%s)
echo -e "${BLUE}ğŸš€ Ø´Ø±ÙˆØ¹ Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ø±Ø§ÛŒ ${domain}...${NC}"

mkdir -p ~/xss-recon/$domain
cd ~/xss-recon/$domain || exit

echo -ne "${BLUE}[1/9] Running assetfinder...${NC}
"
# Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø³Ø§Ø¨â€ŒØ¯Ø§Ù…ÛŒÙ†â€ŒÙ‡Ø§ Ùˆ Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ Ùˆ Ø¨Ø¯ÙˆÙ† Ù¾Ø±ÙˆØªÚ©Ù„
assetfinder --subs-only $domain | sed 's/^https:\/\///' | sed 's/^http:\/\///' | sed 's/^www\.//' | sort -u > subdomains.txt

echo -ne "${BLUE}[2/9] Probing with httpx...${NC}
"
# Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù†Ø¯Ù‡
cat subdomains.txt | httpx -silent | sed 's/^https:\/\///' | sed 's/^http:\/\///' | sed 's/^www\.//' | sort -u > live-subdomains.txt

echo -ne "${BLUE}[3/9] Fetching URLs with gau and waybackurls...${NC}
"
# Ø§Ø³ØªØ®Ø±Ø§Ø¬ URLÙ‡Ø§
cat live-subdomains.txt | while read line; do echo https://$line; done | tee formatted.txt | gau > gau.txt
cat formatted.txt | waybackurls > wayback.txt
cat gau.txt wayback.txt | sort -u > all-urls.txt

echo -ne "${BLUE}[4/9] Filtering parameterized URLs...${NC}
"
# ÙÛŒÙ„ØªØ± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±
cat all-urls.txt | grep "=" > urls-with-params.txt

echo -ne "${BLUE}[5/9] Removing static/API entries...${NC}
"
# Ø­Ø°Ù ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§ØªÛŒÚ© Ùˆ APIÙ‡Ø§ÛŒ Ø®Ø§Øµ
cat urls-with-params.txt | grep -vE "\.(js|css|png|jpg|jpeg|svg|woff|woff2|ttf|eot|ico|gif|map|json|xml|webp|pdf)(\?|$)" | grep -v "/wp-json/" > urls-with-params-clean.txt

echo -ne "${BLUE}[6/9] Normalizing and deduplicating URLs...${NC}
"
# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø­Ø°Ù Ù¾ÙˆØ±Øª 80 Ùˆ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§
cat urls-with-params-clean.txt | sed 's/:80//' | sed 's/^https:\/\///' | sed 's/^http:\/\///' | sed 's/^www\.//' | sort -u > tmp.txt && mv tmp.txt urls-with-params-clean.txt

echo -ne "${BLUE}[7/9] Running ParamSpider for POST discovery...${NC}
"
# Ø§Ø¬Ø±Ø§ÛŒ ParamSpider Ø¨Ø±Ø§ÛŒ POST URLs
paramspider -d $domain > post-links.txt

echo -ne "${BLUE}[8/9] Preparing Dalfox options...${NC}
"
# Ø§Ø¬Ø±Ø§ÛŒ Dalfox
read -p "â“ Do you want to run Dalfox for XSS testing? [y/n]: " confirm
if [[ "$confirm" == "y" || "$confirm" == "Y" ]]; then
    echo -e "${YELLOW}ğŸ“˜ Select test level:
  1) Lite - Fast and minimal
  2) Hard - Full mining and DOM"
    read -p "Your choice [1/2]: " level

    echo -e "${YELLOW}ğŸ“˜ Select Dalfox mode:
  1) dalfox url
  2) dalfox file
  3) dalfox pipe
  4) dalfox server
  5) dalfox scan
  6) dalfox --blind"
    read -p "Your choice [1-6]: " mode

    echo -e "${YELLOW}ğŸ“˜ Select attack type:
  1) Reflected XSS
  2) Stored XSS
  3) DOM-Based XSS
  4) Blind XSS
  5) Parameter Analysis
  6) BAV
  7) Encoder Bypass
  8) Polyglot
  9) Context-aware
 10) Reflection Grep
 11) Header Injection
 12) Static File
 13) MIME Sniffing"
    read -p "Your choice [1-13]: " attack

    FLAGS=""
    [[ "$level" == "1" ]] && FLAGS="--skip-mining-all --timeout 5 -w 50"
    [[ "$attack" == "3" ]] && FLAGS="$FLAGS --deep-domxss"
    [[ "$attack" == "6" ]] && FLAGS="$FLAGS --use-bav"
    [[ "$attack" == "4" && "$mode" == "6" ]] && FLAGS="$FLAGS --blind https://$domain/blind-xss-hook"

    case $mode in
        1) cat urls-with-params-clean.txt | while read url; do dalfox url "$url" $FLAGS | tee -a xss-scan-report.txt; done;;
        2) dalfox file urls-with-params-clean.txt $FLAGS | tee xss-scan-report.txt;;
        3) cat urls-with-params-clean.txt | dalfox pipe $FLAGS | tee xss-scan-report.txt;;
        4) dalfox server;;
        5) dalfox scan --file urls-with-params-clean.txt $FLAGS | tee xss-scan-report.txt;;
        6) dalfox file urls-with-params-clean.txt $FLAGS | tee xss-scan-report.txt;;
    esac

    read -p "ğŸ“„ Save HTML report? [y/n]: " htmlreport
    if [[ "$htmlreport" == "y" || "$htmlreport" == "Y" ]]; then
        dalfox file urls-with-params-clean.txt --format html -o dalfox-report.html
        echo -e "${GREEN}ğŸ“„ HTML report saved as dalfox-report.html${NC}"
    fi
else
    echo -e "${YELLOW}â© Dalfox testing skipped.${NC}"
fi
echo -ne "${BLUE}[9/9] Recon finished. Ready for Dalfox.${NC}
"
echo -e "${GREEN}âœ… Ø¨Ø®Ø´ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ ØªØ³Øª Dalfox Ø±Ùˆ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒ.${NC}"
